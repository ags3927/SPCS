{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STNET to SPCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/data/ani/repos/SPCS/test_data/stnet_copy'\n",
    "\n",
    "Path(os.path.join(out_path, 'counts')).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(out_path, 'coords')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "counts_paths = glob('/data/ani/repos/TRIPLEX/data/stnet/ST-cnts/*')\n",
    "coord_paths = glob('/data/ani/repos/TRIPLEX/data/stnet/ST-spotfiles/*')\n",
    "\n",
    "counts_paths.sort()\n",
    "coord_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counts_path, coord_path in zip(counts_paths, coord_paths):\n",
    "    # Read counts as parquet file\n",
    "    counts = pd.read_parquet(counts_path).T\n",
    "    \n",
    "    # Read coordinates as csv file\n",
    "    coords = pd.read_csv(coord_path, sep='\\t')\n",
    "    \n",
    "    # Extract the index column\n",
    "    names = coords['Unnamed: 0'].tolist()\n",
    "    \n",
    "    # From counts, extract the columns that are in names\n",
    "    counts = counts[names]\n",
    "    \n",
    "    # Save the counts as txt file\n",
    "    counts_save_path = os.path.join(out_path, 'counts', Path(counts_path).stem + '.txt')\n",
    "    counts.to_csv(counts_save_path, sep=',')\n",
    "    \n",
    "    # Open the counts file and delete the first character from the first line and save again\n",
    "    with open(counts_save_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines[0] = lines[0][1:]\n",
    "        \n",
    "    with open(counts_save_path, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    \n",
    "    # Remove columns 'pixel_x' and 'pixel_y' from coords\n",
    "    coords = coords.drop(columns=['pixel_x', 'pixel_y'])\n",
    "\n",
    "    # Rename columns\n",
    "    coords.columns = ['', 'coord1', 'coord2']\n",
    "    \n",
    "    # Save the coords as txt file\n",
    "    coords_save_path = os.path.join(out_path, 'coords', Path(counts_path).stem + '.txt')\n",
    "    coords.to_csv(coords_save_path, sep=',', index=False)\n",
    "    \n",
    "    # Open the counts file and delete the first character from the first line and save again\n",
    "    with open(coords_save_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines[0] = lines[0][1:]\n",
    "        \n",
    "    with open(coords_save_path, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPCS to STNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/data/ani/repos/TRIPLEX/data/stnet/ST-cnts-smoothed'\n",
    "\n",
    "Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "counts_paths = glob('/data/ani/repos/TRIPLEX/data/stnet/ST-cnts/*')\n",
    "coord_paths = glob('/data/ani/repos/TRIPLEX/data/stnet/ST-spotfiles/*')\n",
    "smoothed_counts_paths = glob('/data/ani/repos/SPCS/test_data/stnet/smoothed/*')\n",
    "\n",
    "counts_paths.sort()\n",
    "coord_paths.sort()\n",
    "smoothed_counts_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counts_path, coord_path, smoothed_counts_path in zip(counts_paths, coord_paths, smoothed_counts_paths):\n",
    "    counts = pd.read_parquet(counts_path).T\n",
    "    coords = pd.read_csv(coord_path, sep='\\t')\n",
    "    smoothed_counts = pd.read_csv(smoothed_counts_path, sep=',', index_col=0)\n",
    "    \n",
    "    names = coords['Unnamed: 0'].tolist()\n",
    "    \n",
    "    counts[names] = smoothed_counts\n",
    "    \n",
    "    counts_save_path = os.path.join(out_path, Path(counts_path).name)\n",
    "    \n",
    "    # Save the counts as parquet file\n",
    "    counts.T.to_parquet(counts_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPCS to HGGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/data/ani/datasets/indiana_gene_subset'\n",
    "\n",
    "Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "smoothed_counts_paths = glob('/data/ani/repos/smoothing_tools/SPCS/test_data/indiana/smoothed/*')\n",
    "smoothed_counts_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for smoothed_counts_path in tqdm(smoothed_counts_paths):\n",
    "    smoothed_counts = pd.read_csv(smoothed_counts_path, sep=',', index_col=0).T\n",
    "    \n",
    "    # Save counts as npy file\n",
    "    np.save(os.path.join(out_path, 'counts_spcs', Path(smoothed_counts_path).stem + '.npy'), smoothed_counts.values)\n",
    "    \n",
    "    # Save genes as csv file\n",
    "    cols = pd.DataFrame(smoothed_counts.columns)\n",
    "    cols.to_csv(os.path.join(out_path, 'features', Path(smoothed_counts_path).stem + '.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGGE to SPCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/ani/datasets/indiana_gene_subset'\n",
    "out_path = '/data/ani/repos/smoothing_tools/SPCS/test_data/indiana'\n",
    "\n",
    "Path(os.path.join(out_path, 'counts')).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(out_path, 'coords')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "counts_paths = glob(os.path.join(data_path, 'counts', '*'))\n",
    "features_paths = glob(os.path.join(data_path, 'features', '*'))\n",
    "tissue_positions_paths = glob(os.path.join(data_path, 'tissue_positions', '*'))\n",
    "\n",
    "counts_paths.sort()\n",
    "features_paths.sort()\n",
    "tissue_positions_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file names are the same\n",
    "for counts_path, features_path, tissue_positions_path in zip(counts_paths, features_paths, tissue_positions_paths):\n",
    "    assert Path(counts_path).stem == Path(features_path).stem == Path(tissue_positions_path).stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counts_path, features_path, tissue_positions_path in zip(counts_paths, features_paths, tissue_positions_paths):\n",
    "    # Read the data\n",
    "    counts = np.load(counts_path)\n",
    "    features = pd.read_csv(features_path, header=None)\n",
    "    tissue_positions = pd.read_csv(tissue_positions_path, header=None)\n",
    "    \n",
    "    # Only keep the spots that are in the tissue\n",
    "    tissue_positions = tissue_positions[tissue_positions[1]==1]\n",
    "    \n",
    "    counts_df = pd.DataFrame(counts, columns=features[0].tolist())\n",
    "    counts_df.index = tissue_positions[0].tolist()\n",
    "    \n",
    "    # Transpose the counts so that the spots are in the columns\n",
    "    counts_df = counts_df.T\n",
    "    \n",
    "    # Save the counts as text file\n",
    "    counts_save_path = os.path.join(out_path, 'counts', Path(counts_path).stem + '.txt')\n",
    "    counts_df.to_csv(counts_save_path, sep=',', index=True)\n",
    "    \n",
    "    # Open the counts file and delete the first character from the first line and save again\n",
    "    with open(counts_save_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines[0] = lines[0][1:]\n",
    "        \n",
    "    with open(counts_save_path, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "        \n",
    "    # Save the coordinates as text file\n",
    "    coords_save_path = os.path.join(out_path, 'coords', Path(counts_path).stem + '.txt')\n",
    "    coords_df = tissue_positions.drop(columns=[1,4,5])\n",
    "    coords_df.columns = ['', 'coord1', 'coord2']\n",
    "    \n",
    "    coords_df.to_csv(coords_save_path, sep=',', index=False)\n",
    "    \n",
    "    # Open the counts file and delete the first character from the first line and save again\n",
    "    with open(coords_save_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines[0] = lines[0][1:]\n",
    "        \n",
    "    with open(coords_save_path, 'w') as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
